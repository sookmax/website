import { WithRuler } from "./WithRuler";
import { Event } from "./Event";
import * as Picture from "./Picture";
import { ImageGalleryInline } from "./ImageGalleryInline";
import { ExternalLink } from "lucide-react";

export default function Layout({ children }) {
  return (
    <div className="prose dark:prose-invert max-w-none">
      <h1 className="text-4xl font-bold tracking-tight text-zinc-800 dark:text-zinc-100 md:text-5xl">
        <div>Work Experience</div>
      </h1>
      <p className="mt-6 text-base text-zinc-600 dark:text-zinc-400">
        My career path from Jun. 2015 to Oct. 2022&mdash;from when I graduated
        college to when I quit my latest job. The actual web dev career starts
        around 2019-2020&mdash;everything before that is related to data
        analysis career.
      </p>
      <WithRuler className="mt-12">{children}</WithRuler>
    </div>
  );
}

<Event from="2021-06" to="2022-10">

<h2 className="flex flex-col">
  <span className="flex space-x-1">
    <span>Frontent Developer - 3i</span>
    <a href="https://www.3i.ai/" target="_blank" className="flex items-center">
      <ExternalLink className="w-5 h-5" />
    </a>
  </span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

</Event>

<Event from="2020-12" to="2021-06">

<h2 className="flex flex-col">
  <span className="flex space-x-1">
    <span>Frontent Developer - Dawinproperty</span>
    <a href="https://dawin.xyz/" target="_blank" className="flex items-center">
      <ExternalLink className="w-5 h-5" />
    </a>
  </span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/dawin-1.png",
      "/dawin-2.png",
      "/dawin-3.png",
      "/dawin-4.png",
      "/dawin-5.png",
      "/dawin-6.png",
    ]}
  >
    <a href="https://dawin.xyz/" target="_blank" className="text-primary-dark">
      {"https://dawin.xyz/"}
    </a>
  </ImageGalleryInline>
</div>

Having only worked at B2B companies, I wanted to work on products that targeted customers more directly (B2C), so I was glad to join this company since it runs a real estate website where potential home buyers can come and interact with various information on a 2D area map. The website is also useful for home sellers because they can post their home for sale.

The website was a React single page application (SPA) using [React Router](https://reactrouter.com/en/main) as a client-side router and [Redux-Saga](https://redux-saga.js.org/) to manage the global state. And the map itself was provided by a third-party service called [kakaomap Maps API](https://apis.map.kakao.com/). In fact one of the first things I did after joining the company was to add some additional features to the map such as satellite view and a distance measuring tool using kakaomap APIs.

I also had a chance to work with web forms&mdash;the form that sellers need to fill out to post their home on the website. Particularly, there were a lot of information in the form that the user needed to fill in, so naturally it was divided into multiple steps, yet it didn't really have mechanism to save progress as a draft&mdash;which means the user will loose their form states and need to fill out the form again(!) if something went wrong while navigating back and forth between steps. So we decided to add a draft state of the application in the database and save the draft whenever the user navigates to next/previous steps. It was more of a backend work, but fortunately the backend app was a node.js app (JavaScript yay) and I had some prior experience in SQL (our database was MySQL) so I didn't have too much trouble implementing [Express](https://expressjs.com/) api handlers and writing SQL queries for them.

Speaking of node.js, one of the tasks I was assigned to had to do with creating an API endpoint&mdash;that was separate from our backend node.js app&mdash;that collects and aggregates some third-party data we wanted to use on our website. I remember I had a hard time implementing the 'aggregation' part of the logic. It was in essence similar to nested SQL GROUP BY statement followed by averaging or summing the group members (something like [Array.reduce()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce))&mdash;something that I did pretty regularly when I was a data analyst in either R, Python, or SQL. However it was surprisingly harder to do the same thing in JavaScript. I vaguely remember I came up with some unnecessarily convoluted recursive groupby() function and a reducer that used a lot of [Array.flat()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/flat) and [Array.flatMap()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/flatMap).

Back to the frontend app, I want to mention that this React app was also suffering from bloated and unmanageable `useEffect()`s&mdash;something that I had experienced in the previous company as well. The problem was worse here because there were a few giant React components that many other components depended on that were doing too much work&mdash;which means these massive React components contained too much (nested) conditionals and business logic. I was very surprised that this was actually an intentional design choice made by one of our main frontend developers&mdash;she favored to have these bloated components around, and everytime I needed to add something new, she would point me to put more logic in them. I, on the other hand, wanted to break down these monster components to smaller pieces so that we have more modular and independent components. Unfortunately, everytime I suggested decomposing big components or attempted at doing it, she would strongly oppose it&mdash;she was a firm believer that things were easier to control and manage by having few big components that do a lot of work. This fundamental difference in view actually was a major reason I left the company.

</Event>

<Event from="2019-04" to="2020-12">

<h2 className="flex flex-col">
  <span className="flex space-x-1">
    <span>Software Engineer - AgileSoDA</span>
    <a
      href="http://www.agilesoda.com/"
      target="_blank"
      className="flex items-center"
    >
      <ExternalLink className="w-5 h-5" />
    </a>
  </span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/agilesoda-swe-1.png",
      "/agilesoda-swe-2.png",
      "/agilesoda-swe-3.png",
      "/agilesoda-swe-4.png",
      "/agilesoda-swe-5.png",
      "/agilesoda-swe-6.png",
      "/agilesoda-swe-7.png",
      "/agilesoda-swe-8.png",
      "/agilesoda-swe-9.png",
      "/agilesoda-swe-10.png",
      "/agilesoda-swe-11.png",
      "/agilesoda-swe-12.png",
      "/agilesoda-swe-13.png",
      "/agilesoda-swe-14.png",
      "/agilesoda-swe-15.png",
      "/agilesoda-swe-16.png",
      "/agilesoda-swe-17.png",
      "/agilesoda-swe-18.png",
      "/agilesoda-swe-19.png",
      "/agilesoda-swe-20.png",
      "/agilesoda-swe-21.png",
      "/agilesoda-swe-22.png",
      "/agilesoda-swe-23.png",
      "/agilesoda-swe-24.png",
      "/agilesoda-swe-25.png",
      "/agilesoda-swe-26.png",
      "/agilesoda-swe-27.png",
      "/agilesoda-swe-28.png",
      "/agilesoda-swe-29.png",
      "/agilesoda-swe-30.png",
      "/agilesoda-swe-31.png",
      "/agilesoda-swe-32.png",
      "/agilesoda-swe-33.png",
      "/agilesoda-swe-34.png",
      "/agilesoda-swe-35.png",
      "/agilesoda-swe-36.png",
      "/agilesoda-swe-37.png",
      "/agilesoda-swe-38.png",
      "/agilesoda-swe-39.png",
      "/agilesoda-swe-40.png",
      "/agilesoda-swe-41.png",
      "/agilesoda-swe-42.png",
      "/agilesoda-swe-43.png",
      "/agilesoda-swe-44.png",
    ]}
  />
</div>

Having shown my interest and aptitude towards computer systems and software development many times, I was finally granted a transfer to the engineering team of the company as I wished. We had a B2B SaaS product whose target users were themselves data analysts at our client companies, and I was excited to bring my perspective as a former data analyst&mdash;more of the actual user's perspective I'd say&mdash;to the product. Old me back then was fairly confident that he could learn new things relatively quickly and get himself up to speed fast enough to handle the tasks thrown at him in the near future, but little did he knew what kind of challenges were waiting for him&mdash;as the old saying goes, ignorance is bliss ðŸ˜‚.

I mean, I had my fair share of coding in R and Python (albeit not software production code) and some sys admin / IT infra stuff on Linux, but now we were talking about a full-fledged backend application written in Java, a frontend written in React (class components), a MariaDB for database, and underneath all that, a big (frankly unnecessary) infrastructure to serve and manage all these services, namely, Docker and Kubernetes. I remember putting in around 3-4 extra hours after work for months digging through the codebases and various docs websites&mdash;[Kubernetes](https://kubernetes.io/docs/home/), [Eclipse Vert.x](https://vertx.io/docs/), [React](https://legacy.reactjs.org/docs/getting-started.html), [MariaDB](https://mariadb.com/kb/en/), [Dockerfile](https://docs.docker.com/engine/reference/builder/), etc.&mdash;to make myself useful faster.

Slowly but gradually I became more self-sufficient as the tasks assigned to me evolved from supporting minor feature developments to implementing major independent feature. As my understanding of the implementation of the product grew however, I began to question the decisions that had been made in regard to our tech stack&mdash;such as "Was involving Kubernetes absolutely necessary, especially when it's suffice to have a single machine to run all our services?", "If managing docker containers was a concern why couldn't we just go with docker compose instead of Kubernetes?", "Why this esoteric framework called Vert.x for our backend?" or "Why do we need a distributed storage solution such as GlusterFS when we don't really have any storage scaling issue yet?"

I was especially disturbed by the complexity of the frontend codebase which was originally bootstrapped by [this boilerplate project](https://github.com/erikras/react-redux-universal-hot-example). The starter project was meant for the web apps that needed both server-side and client-side rendering techniques, but we weren't using the server-side rendering at all&mdash;we were only rendering the UI client-side, which means a static server would've been sufficient to serve the skeleton html, client-side JavaScripts, and other static assets, but instead we had a full-blown node.js server and all the other complexity that came with isomorphic rendering technique. I was so frustrated with the frontend codebase that I gave a presentation internally in the company to raise my concern.

Shortly after though, I was given an opportunity to implement the UI for the new product from scratch. I was in such high spirits in the beginning because this time I was able to use [React hooks](https://legacy.reactjs.org/docs/hooks-intro.html) (every React component in the existing codebase was a class component). While I tried hard not to include any unnecessary dependencies, I relied heavily on [Blueprint.js](https://blueprintjs.com/docs/) and [TanStack Table](https://tanstack.com/table/v8/docs/guide/introduction) for complex components such as forms, dialogs, and tables. For charts and graphs, I used [Apache ECharts](https://echarts.apache.org/en/api.html#echarts) and [react-chartjs-2](https://react-chartjs-2.js.org/).

Despite my initial excitement for React hooks however, I noticed some of my [useEffect()](https://legacy.reactjs.org/docs/hooks-effect.html) hooks were getting incredibly complicated to the point where I was no longer able to analyze what they did without slapping in `console.log()` here and there and checking the runtime behaviors. I failed to manage the complexity of some React components, and I also probably overused `useEffect()` hook unnecessarily due to my lack of understanding of React rendering model at that time. I also must admit that at the end I was very afraid to touch some of my `useEffect()`s because they were so brittle that the slightest change would break something unexpectedly&mdash;my own poor usage of `useEffect()`s left me traumatized and made me question the usefulness of React hooks for a while ðŸ˜….

</Event>

<Event from="2017-04" to="2019-04">

<h2 className="flex flex-col">
  <span className="flex space-x-1">
    <span>Data Analyst - AgileSoDA</span>
    <a
      href="http://www.agilesoda.com/"
      target="_blank"
      className="flex items-center"
    >
      <ExternalLink className="w-5 h-5" />
    </a>
  </span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/agilesoda-da-1.png",
      "/agilesoda-da-2.png",
      "/agilesoda-da-3.png",
      "/agilesoda-da-4.png",
      "/agilesoda-da-5.png",
      "/agilesoda-da-6.png",
      "/agilesoda-da-7.png",
    ]}
  />
</div>

This was the third and the last job I had as a data analyst. This particular company was where I was able to switch position to software engineer later, which evolved into an entirely frontend position at the end. Almost half of my time here as a data analyst, I was involved in the projects that attempted to predict the future stock prices (or the directions of them), which&mdash;not surprisingly&mdash;failed spectacularly, resulting in the models whose prediction accuracies were around 50% (which is equivalent of a coin toss ðŸ˜‚). In hindsight, we were trying too hard to find even a smallest signal in the stock market data, which I believe is inherently random with no meaningful pattern to detect and capture with a model. Related, it was this time around that I started to lose interset in data analysis as a profession. I felt especially frustrated when the data I was analyzing didn't really have any meaningful or interesting patterns&mdash;in that case, no amount of work from my end could turn the result around. It was basically garbage in, garbage out.

In contrast, my interest towards computer system and software engineering were growing steadily while doing stuff like shell scripting, spawning a process, killing a malfunctioning process, cron jobs, ssh tunneling, etc. that were necessary for our data analysis team to do their job&mdash;I mostly volunteered to do these kind of peripheral works because they were fun to do.

One particular project I was involved had something to do with data migration&mdash;from the client's Oracle database to our data analytics cluster, which used [hdfs](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) as its default file system. It sounds daunting, but the tool like [sqoop](https://sqoop.apache.org/) helped me move the entire Oracle database and tables to hdfs [as Hive tables](https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.4.2/bk_dataintegration/content/using_sqoop_to_move_data_into_hive.html) without too much challenges. I also needed to port the Oracle SQL queries we were using in the project to HiveQL so that our queries still work after the migration to Hive was done. This job was supposed to be taken care of by the engineer in the project, but his workload was too much at that time that he couldn't come to the project site (client's office where we&mdash;data analysts&mdash;were working alongside the client) as much as we wanted him to, so I ended up doing it myself. This kind of episode actually helped me later to appeal to the management that I wanted to move to an engineering position.

<h3>Personal Project - Mini Hadoop Cluster</h3>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/hadoop-1.png",
      "/hadoop-2.png",
      "/hadoop-3.png",
      "/hadoop-4.png",
      "/hadoop-5.png",
      "/hadoop-6.png",
      "/hadoop-7.png",
      "/hadoop-8.png",
      "/hadoop-9.png",
      "/hadoop-10.png",
      "/hadoop-11.png",
      "/hadoop-12.png",
      "/hadoop-13.png",
      "/hadoop-14.png",
      "/hadoop-15.png",
      "/hadoop-16.png",
      "/hadoop-17.png",
      "/hadoop-18.png",
      "/hadoop-19.png",
      "/hadoop-20.png",
      "/hadoop-21.png",
      "/hadoop-22.png",
      "/hadoop-23.png",
    ]}
  />
</div>

Circa 2016-2017, I was more than fascinated by the concept of distributed computing represented primarily by [Hadoop](https://hadoop.apache.org/) and [Spark](https://spark.apache.org/). The very idea that multiple computers connected through a network (a cluster) work together to represent a single entity to handle computations that is beyond any single machine was just phenomenal to me.

I could've just spun up multiple virtual machines on my laptop to provision and play with a Hadoop cluster, but instead I decided to create one that's composed of physically separated machines&mdash;which, in hindsight, was totally unnecessary and kind of a waste of money, but at the moment I thought it'd be more fun that way... and actually it was ðŸ˜„. Although I didn't really use this cluster of machines in any meaningful way after I was done configuring, this cluster would later be transformed into my [kubernetes](https://kubernetes.io/) dev cluster and serve me well at work.

</Event>

<Event from="2016-09" to="2017-04">

<h2 className="flex flex-col">
  <span className="flex space-x-1">
    <span>Data Analyst - Mobigen</span>
    <a
      href="http://www.mobigen.com/eng/index.php"
      target="_blank"
      className="flex items-center"
    >
      <ExternalLink className="w-5 h-5" />
    </a>
  </span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/mobigen-1.png",
      "/mobigen-2.png",
      "/mobigen-3.png",
      "/mobigen-4.png",
      "/mobigen-5.png",
      "/mobigen-6.png",
      "/mobigen-7.png",
      "/mobigen-8.png",
      "/mobigen-9.png",
      "/mobigen-10.png",
      "/mobigen-11.png",
      "/mobigen-12.png",
      "/mobigen-13.png",
      "/mobigen-14.png",
      "/mobigen-15.png",
      "/mobigen-16.png",
      "/mobigen-17.png",
      "/mobigen-18.png",
      "/mobigen-19.png",
      "/mobigen-20.png",
      "/mobigen-21.png",
      "/mobigen-22.png",
      "/mobigen-23.png",
    ]}
  />
</div>

It was my second job as a data analyst. I remember this company had much bigger engineering department compared to my first company, probably because it had a B2B SaaS product or two already being sold when I joined&mdash;they were putting together a data analytics team for the first time when they hired me. I didn't know the details of the products they had, but the software engineers developing the products used [Python](https://www.python.org/) as their primary language of choice (or so they said since I never saw or touched the actual product codebases). Naturally, there were quite a lot of competent Python programmers in the company including the then CTO, who led a Python study group for new hires, which I was a part of, once a week. There were also a few data analysts that had already been working in the company by the time I joined, and they were also using mostly Python for their tasks. So it was natural for me to learn and explore data analysis tools in Python ecosystem&mdash;I remember using tools like [scikit-learn](https://scikit-learn.org/stable/), [pandas](https://pandas.pydata.org/), and [matplotlib](https://matplotlib.org/).

To me, 2016 and 2017 were prime time for machine learning. Well, I think it wasn't just me, but there were a lot of excitement in general around machine learning in that time period, thanks to people like [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng), [Christopher Bishop](https://en.wikipedia.org/wiki/Christopher_Bishop), and [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) giving public and accessible lectures and talks. It was _the_ period of artificial neural networks, [deep learning](https://www.deeplearningbook.org/), and [TensorFlow](https://www.tensorflow.org/) as I remember it.

</Event>

<Event from="2015-07" to="2016-08">

<h2 className="flex flex-col">
  <span>Data Analyst - NowDream</span>
  <span className="font-normal text-base">Seoul, South Korea</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/nowdream-1.png",
      "/nowdream-2.png",
      "/nowdream-3.png",
      "/nowdream-4.png",
      "/nowdream-5.png",
      "/nowdream-6.png",
    ]}
  />
</div>

This was my first job as a data analyst fresh out of college. With memory still relatively fresh (which is no longer the case), I remember I tried very hard to apply what I'd learned at school to the tasks I was assigned.

It was also this time around that I started developing interest towards computer system behind the scene. You see, when I was in school, my typical IDE was a desktop app called [RStudio](https://posit.co/products/open-source/rstudio/) and that was it. But now my IDE was running on [the server](https://posit.co/products/open-source/rstudio-server/), and I accessed my RStudio through the browser (as in the form of web app). At that time, this RStudio "server" and the web UI that I can connect and use through the browser felt like dark magic almost ðŸ¤¯. And to control this misterious server&mdash;which was nothing but a desktop PC quietly sitting in the corner of the office&mdash;I had to connect to it through something called an SSH client (I mostly used [PuTTY](https://www.putty.org/) back then), which would instantly start a Bash session if successfully connected&mdash;it was the first time I interacted with a Linux OS. I was quite fascinated by shell environment of Linux that I would randomly connect to the server just to play around with Bash and its utilites.

It was also the period where big data hype was at its peak, so I had a chance to play a little bit with [Hadoop](https://hadoop.apache.org/) and [Hive](https://hive.apache.org/)&mdash;as a matter of fact, Hive was my first experience with SQL.

</Event>

<Event date="2015-06">

<h2 className="flex flex-col">
  <span>Graduated UCLA with Statistics, B.S.</span>
  <span className="font-normal text-base">Los Angeles, CA</span>
</h2>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={["/graduation-1.jpg", "/graduation-2.jpg", "/graduation-3.jpg"]}
  />
</div>

<h3>
  Some group project slides I've found still sitting in my google drive ðŸ˜‚
</h3>

<div className="space-y-10">
<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/group-project-1.png",
      "/group-project-2.png",
      "/group-project-3.png",
      "/group-project-4.png",
      "/group-project-5.png",
      "/group-project-6.png",
      "/group-project-7.png",
      "/group-project-8.png",
    ]}
  />
</div>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/group-project-9.png",
      "/group-project-10.png",
      "/group-project-11.png",
      "/group-project-12.png",
      "/group-project-13.png",
      "/group-project-14.png",
      "/group-project-15.png",
    ]}
  />
</div>

<div className="shadow-md shadow-zinc-200/70 dark:shadow-zinc-950/50 not-prose">
  <ImageGalleryInline
    imageUrls={[
      "/group-project-16.png",
      "/group-project-17.png",
      "/group-project-18.png",
      "/group-project-19.png",
      "/group-project-20.png",
      "/group-project-21.png",
      "/group-project-22.png",
      "/group-project-23.png",
      "/group-project-24.png",
    ]}
  />
</div>
</div>

</Event>
